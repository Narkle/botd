# SURF2017
# File: FlowParser.py
# Created: 6/26/17
# Author: Stephanie Ding
# Description:
# This module parses NetFlow files generated by a variety of applications.
# There are functions for parsing the CTU datset in particular which take the
# list of infected IPs as an argument and output the required array, etc.

import constants
import csv
import numpy as np
import random
import os
import glob
from sklearn.preprocessing import StandardScaler
from sklearn.externals import joblib
from sklearn.metrics import confusion_matrix, accuracy_score

# Argus NetFlow parser

# These fields define which features comprise the flow ID
ARGUS_FLOW_ID = ['SrcAddr', 'Sport', 'DstAddr', 'Dport', 'Proto']

# These fields define which features comprise the flow feature vector
ARGUS_FIELDS = ['sTtl', 'dTtl', 'SrcPkts', 'DstPkts', 'SrcBytes', 'DstBytes', 'SrcLoad', 'DstLoad', 'Dir',
                'SIntPkt', 'DIntPkt', 'SIntPktAct', 'DIntPktAct', 'SIntPktIdl', 'DIntPktIdl', 'SrcJitter',
                'DstJitter', 'SrcJitAct', 'DstJitAct', 'State', 'sMaxPktSz', 'dMaxPktSz', 'sMinPktSz',
                'dMinPktSz', 'Dur', 'Rate', 'SrcRate', 'DstRate', 'RunTime', 'Mean', 'Sum', 'Min',
                'Max', 'Load', 'pSrcLoss', 'pDstLoss']

# Numeric encoding for states and direction, which are non-numeric discrete parameters
ARGUS_DIRECTIONS = {'->':0, '?>':1, '<-':2, '<?':3, '<->':4, '<?>':5, 'who':6}

ARGUS_STATES = {'ACC': 12, 'DCE': 23, 'REQ': 9, 'URFIL': 17, 'RED': 8, 'CON': 2, 'URHPRO': 6,
                'ECO': 5, 'TXD': 15, 'ECR': 14, 'NNS': 16, 'SRC': 22, 'URF': 20, 'CLO': 19,
                'STA': 0, 'URN': 10, 'URO': 21, 'URH': 11, 'URNPRO': 24, 'RST': 1, 'URP': 7,
                'FIN': 3, 'INT': 4, 'NRS': 18, 'RSP': 13}

'''
A flow ID is represented as an array of flow ID features in the order they were parsed.
For an Argus file, the first field will be source IP address and the 3rd field will be
destination IP address. Return this as a tuple.
'''
def get_src_dst(flow_id):
    return (flow_id[0], flow_id[2])

'''
Given a dictionary that represents a flow (a line parsed with csvreader), return an
array of the flow ID fields only, in the specified order.
'''
def get_argus_flow_id(flow):
    return (flow["SrcAddr"], flow["Sport"], flow["DstAddr"], flow["Dport"], flow["Proto"]) #, flow["Dir"].strip())

'''
Parses a single line of an Argus Netflow file.
Given a list of features to parse (default is ARGUS_FIELDS in many usage cases with uses
all 36 parameters) and return both the flow ID, and a numeric feature vector as a tuple.
'''
def parse_argus_flow(flow, features_list):
    features = []
    for field in features_list:
        if field == 'Dir':
            features.append(ARGUS_DIRECTIONS[flow['Dir'].strip()])
        elif field == 'State':
            features.append(ARGUS_STATES[flow['State']])
        else:
            try:
                features.append(float(flow[field]))
            except Exception, e:
                features.append(0)
    return (get_argus_flow_id(flow), features)

'''
Parses an entire Argus NetFlow file given the specified feature list (ARGUS_FIELDS is
the default used in most cases, but a custom subset can be defined)
Let n be the number of flows in the file, then this function will output a
(n x 5) array of the flow IDs and a (n x 36) array of the feature vectors.
The flow IDs and the feature vectors will correspond in enumeration, i.e. flows[1]
is the ID for xs[1].
Returns (flows, xs) as a tuple
'''
def parse_binetflow(filename, features_list=ARGUS_FIELDS):
    flows = []
    xs = []

    with open(filename) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                continue

            try:
                flow_id, x = parse_argus_flow(flow, features_list)
            except Exception, e:
                continue

            flows.append(flow_id)
            xs.append(x)

    xs = np.nan_to_num(xs)
    return (flows, xs)

'''
Parses an Argus Netflow file with a corresponding labelled NetFlow file (i.e.
in the case of the CTU-13 datasets). Takes labelled NetFlow as first parameter,
non-labelled NetFlow (with more features) as second parameter, and optional
list of arguments to use for generating the feature vectors.
Returns flow IDs, xs, and ys. The ys are a binary array (n x 1) where the value is
1 if it is a botnet flow, and 0 if it is a non-botnet flow. Splits the entire
dataset into 30% training and 70% validation.
Will process the values to turn all NaN values in the arrays to 0.
'''
def parse_argus(labels, filename, features_list=ARGUS_FIELDS):
    random.seed(0)

    botnet_flows = set()
    normal_flows = set()
    background_flows = set()

    print("Opening labelled Argus binetflow...")
    total = 0
    skipped = 0
    with open(labels) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            #print(flow)
            total += 1
            flow_id = get_argus_flow_id(flow)
            #flow_id = (flow_id[0], flow_id[4])

            if 'From-Botnet' in flow['Label']:
                botnet_flows.add(flow_id)
            elif 'Normal' in flow['Label']:
                normal_flows.add(flow_id)
            elif 'Background' in flow['Label']:
                background_flows.add(flow_id)
            else:
                skipped += 1
                continue

    print("Parsed total " + str(total) + " flows, skipped " + str(skipped))
    print("Total " + str(len(botnet_flows)) + " unique From-Botnet flow IDs")
    print("Total " + str(len(normal_flows)) + " unique Normal flow IDs")
    print("Total " + str(len(background_flows)) + " unique Background flow IDs")

    print("Opening extended Argus binetflow...")

    total = 0
    botnet_count = 0
    skipped = 0

    flow_ids = []
    xs = []
    ys = []

    test_flow_ids = []
    test_xs = []
    test_ys = []

    with open(filename) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            total += 1

            if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                skipped += 1
                continue

            try:
                flow_id, x = parse_argus_flow(flow, features_list)
                #flow_id = (flow_id[0], flow_id[4])
            except Exception, e:
                skipped += 1
                continue

            if flow_id in botnet_flows:
                botnet_count += 1
                xs.append(x)
                ys.append([1]) #ys.append([1])
                flow_ids.append(flow_id)
            elif flow_id in normal_flows: # or flow_id in background_flows:
                xs.append(x)
                ys.append([0])
                flow_ids.append(flow_id)
            elif flow_id in background_flows:
                r = random.randint(0, 10)
                if r == 0:
                    xs.append(x)
                    ys.append([0]) #ys.append([0])
                    flow_ids.append(flow_id)
                else:
                    test_xs.append(x)
                    test_ys.append([0]) #test_ys.append([0])
                    test_flow_ids.append(flow_id)
            else:
                # src, dst = get_a_src_dst(flow_id)
                # if src in constants.DATASET_10_INFECTED_HOSTS:
                #     xs.append(x)
                #     ys.append([1])
                #     flow_ids.append(flow_id)
                # else:
                #     xs.append(x)
                #     ys.append([0])
                #     flow_ids.append(flow_id)
                skipped += 1
                continue

    print("Parsed total " + str(total) + " flows, skipped " + str(skipped))
    print("Final training dataset contains " + str(len(xs)) + " flows, with " + str(botnet_count) + " botnet flows")
    print(str(len(test_xs)) + " non-botnet flows added to testing set")

    xs = np.nan_to_num(xs)
    ys = np.array(ys)
    test_xs = np.nan_to_num(test_xs)
    test_ys = np.array(test_ys)

    all_data = list(zip(flow_ids, xs, ys))
    random.shuffle(all_data)
    flow_ids, xs, ys = zip(*all_data)

    test_partition_size = int(0.3 * len(xs))

    temp = 0
    for i in ys[:test_partition_size]:
        if i[0] == 1:
            temp += 1

    print(str(temp) + " botnet flows in testing set")


    test_flow_ids += flow_ids[:test_partition_size]
    test_xs = np.concatenate((test_xs, xs[:test_partition_size]))
    test_ys = np.concatenate((test_ys, ys[:test_partition_size]))
    #
    # test_flow_ids = flow_ids[:test_partition_size]
    # test_xs = xs[:test_partition_size]
    # test_ys = ys[:test_partition_size]

    train_flows = flow_ids[test_partition_size:]
    train_xs = xs[test_partition_size:]
    train_ys = ys[test_partition_size:]

    print("Number of training flows (70% of training dataset): " + str(len(train_xs)))
    print("Number of testing flows (30% of training dataset + non-botnet flows): " + str(len(test_xs)))

    #p = np.random.permutation(len(xs))
    # all_data = list(zip(flow_ids, xs, ys))
    # random.shuffle(all_data)
    # flow_ids, xs, ys = zip(*all_data)

    return (train_flows, train_xs, train_ys, test_flow_ids, test_xs, test_ys)

'''
A simpler version of parse_argus that just returns the xs, ys and flow IDs.
Will also process the values and turn NaN to 0 before returning.
This returns xs, ys and flow ID labels that can be immediately used for
training purposes e.g. Tensorflow or scikit-learn.
'''
def parse_ctu(filename, labels, features_list=ARGUS_FIELDS, sample_rate=0):
    botnet_flows = set()
    normal_flows = set()
    background_flows = set()

    print("Opening labelled Argus binetflow...")
    total = 0
    skipped = 0
    with open(labels) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            total += 1
            flow_id = get_argus_flow_id(flow)

            if 'From-Botnet' in flow['Label']:
                botnet_flows.add(flow_id)
            elif 'Normal' in flow['Label']:
                normal_flows.add(flow_id)
            elif 'Background' in flow['Label']:
                background_flows.add(flow_id)
            else:
                skipped += 1
                continue

    print("Parsed total " + str(total) + " flows, skipped " + str(skipped))
    print("Total " + str(len(botnet_flows)) + " unique From-Botnet flow IDs")
    print("Total " + str(len(normal_flows)) + " unique Normal flow IDs")
    print("Total " + str(len(background_flows)) + " unique Background flow IDs")

    print("Opening extended Argus binetflow...")

    total = 0
    botnet_count = 0
    skipped = 0

    flow_ids = []
    xs = []
    ys = []

    test_flow_ids = []
    test_xs = []
    test_ys = []

    with open(filename) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            #total += 1

            if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                skipped += 1
                continue

            try:
                flow_id, x = parse_argus_flow(flow, features_list)
            except Exception, e:
                skipped += 1
                continue

            r = random.randint(0, sample_rate)
            if r == 0:
                total += 1

                if flow_id in botnet_flows:
                    botnet_count += 1
                    xs.append(x)
                    ys.append([1])  # ys.append([1])
                    flow_ids.append(flow_id)
                elif flow_id in normal_flows or flow_id in background_flows:
                    xs.append(x)
                    ys.append([0])
                    flow_ids.append(flow_id)
                else:
                    skipped += 1
                    continue

    print("Parsed total " + str(total) + " flows, skipped " + str(skipped))
    print("Final training dataset contains " + str(len(xs)) + " flows, with " + str(botnet_count) + " botnet flows")

    xs = np.nan_to_num(xs)
    ys = np.array(ys)

    return (flow_ids, xs, ys)

'''
A batch processing class. Initializes with a labelled binetflow file on botnet/non-botnet traffic and then processes
a whole folder.
'''
class ArgusBatchSession:
    window_length = 0
    overlap_length = 0
    botnet_flows = set()
    normal_flows = set()
    background_flows = set()

    '''
    On initialization of the session, load the file with labels to generate the traffic profile and keep sets of
    which IDs are botnet IDs, normal IDs, and background IDs. Also take in the duration of each window (in seconds)
    and the number of seconds overlap each window has with the previous window.
    '''
    def __init__(self, labels, features_list, window_length, overlap_length):
        self.window_length = window_length
        self.overlap_length = overlap_length
        self.features_list = features_list

        print("Opening labelled Argus binetflow...")
        total = 0
        skipped = 0

        with open(labels) as f:
            reader = csv.DictReader(f)
            for flow in reader:
                total += 1
                flow_id = get_argus_flow_id(flow)

                if 'From-Botnet' in flow['Label']:
                    self.botnet_flows.add(flow_id)
                elif 'Normal' in flow['Label']:
                    self.normal_flows.add(flow_id)
                elif 'Background' in flow['Label']:
                    self.background_flows.add(flow_id)
                else:
                    skipped += 1
                    continue

        print("Parsed total " + str(total) + " flows, skipped " + str(skipped))
        print("Total " + str(len(self.botnet_flows)) + " unique From-Botnet flow IDs")
        print("Total " + str(len(self.normal_flows)) + " unique Normal flow IDs")
        print("Total " + str(len(self.background_flows)) + " unique Background flow IDs")

    '''
    Takes in a scikit-learn classifier as a parameter, trains the classifier on the provided filename
    and saves it as a pickle file to the provided save path. n_bg is the number of background samples
    to use in the training dataset and n_botnet is the number of botnet samples to use in the training
    dataset.
    '''
    def train_model_on_file(self, clf, n_bg, n_botnet, save_path, filename):
        skipped = 0

        botnet_xs = []
        botnet_ys = []

        bg_xs = []
        bg_ys = []


        botnet_i = 0
        bg_i = 0

        with open(filename) as f:
            reader = csv.DictReader(f)
            for flow in reader:
                try:
                    if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                        skipped += 1
                        continue

                    flow_id, x = parse_argus_flow(flow, self.features_list)
                except Exception, e:
                    skipped += 1
                    continue

                if flow_id in self.botnet_flows:
                    botnet_i += 1
                    if botnet_i < n_botnet:
                        botnet_xs.append(x)
                        botnet_ys.append(1)
                    else:
                        j = random.randint(1, botnet_i)
                        if j < n_botnet:
                            botnet_xs[j - 1] = x
                elif flow_id in self.normal_flows or flow_id in self.background_flows:
                    bg_i += 1
                    if bg_i < n_bg:
                        bg_xs.append(x)
                        bg_ys.append(0)
                    else:
                        j = random.randint(1, bg_i)
                        if j < n_bg:
                            bg_xs[j - 1] = x
                else:
                    continue

        train_x = np.concatenate((bg_xs, botnet_xs))
        train_y = np.concatenate((bg_ys, botnet_ys))

        train_x = np.nan_to_num(train_x)
        train_y = np.array(train_y)

        all_data = list(zip(train_x, train_y))
        random.shuffle(all_data)
        train_x, train_y = zip(*all_data)

        print("Skipped: " + str(skipped) + ", Training on dataset of size: " + str(len(train_x)))

        clf = clf.fit(train_x, train_y)
        joblib.dump(clf, save_path)


    '''
    Train a scikit-learn classifier on all files in a folder of NetFlows. We assume these NetFlows are named
    as 1.binetflow, 2.binetflow, ... etc. (can be generated with PcapTools.py) n_bg and n_botnet will be divided 
    by the number of files in the folder such that an equal number of samples are obtained from each file. Also 
    has parameters defining between which NetFlow file the infected botnet is running (netflows outside the
    infection range are assumed to contain only benign traffic)
    '''
    def train_model_on_folder(self, clf, n_bg, n_botnet, save_path, folder_path, infection_start, infection_end):
        random.seed(0)
        owd = os.getcwd()

        os.chdir(folder_path)
        windows = sorted(glob.glob("[0-9]*.binetflow"), key=lambda x: int(x.split(".")[0]))
        first_window = int(windows[0].split(".")[0])
        last_window = int(windows[-1].split(".")[0])

        # Use reservoir sampling

        bg_samples = int(n_bg / (last_window - first_window))
        botnet_samples = int(n_botnet / (infection_end - infection_start))

        train_x = []
        train_y = []

        for window in windows:
            print("window: " + window)
            bg_xs = []
            botnet_xs = []

            botnet_i = 0
            bg_i = 0

            with open(window) as f:
                n = int(window.split(".")[0])

                reader = csv.DictReader(f)
                for flow in reader:
                    try:
                        if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                            continue

                        flow_id, x = parse_argus_flow(flow, self.features_list)
                    except Exception, e:
                        continue

                    if n < infection_start or n > infection_end or (flow_id in self.background_flows or flow_id in self.normal_flows):
                        bg_i += 1
                        if bg_i < bg_samples:
                            bg_xs.append(x)
                        else:
                            j = random.randint(1, bg_i)
                            if j < bg_samples:
                                bg_xs[j-1] = x
                    elif flow_id in self.botnet_flows and n >= infection_start and n <= infection_end:
                        botnet_i += 1
                        if botnet_i < botnet_samples:
                            botnet_xs.append(x)
                        else:
                            j = random.randint(1, botnet_i)
                            if j < botnet_samples:
                                botnet_xs[j-1] = x
                    else:
                        continue

            bg_ys = np.zeros(len(bg_xs), dtype=int)
            if train_x == []:
                train_x = bg_xs
                train_y = bg_ys
            else:
                train_x = np.concatenate((train_x, bg_xs))
                train_y = np.concatenate((train_y, bg_ys))

            if botnet_xs != []:
                botnet_ys = np.ones(len(botnet_xs), dtype=int)
                train_x = np.concatenate((train_x, botnet_xs))
                train_y = np.concatenate((train_y, botnet_ys))


        train_x = np.nan_to_num(train_x)
        train_y = np.array(train_y)

        all_data = list(zip(train_x, train_y))
        random.shuffle(all_data)
        train_x, train_y = zip(*all_data)

        # scaled_xs = StandardScaler().fit_transform(train_x)

        print("Training on dataset of size: " + str(len(train_x)))

        clf = clf.fit(train_x, train_y)
        os.chdir(owd)
        joblib.dump(clf, save_path)

    '''
    Test the performance of a specified model on every file in a folder, specifying when the infection begins.
    Will output accuracy metrics for each file.
    '''
    def test_model_on_folder(self, clf, folder_path, infection_start):
        os.chdir(folder_path)

        for window in sorted(glob.glob("[0-9]*.binetflow"), key=lambda x: int(x.split(".")[0])):
            flow_ids = []
            xs = []
            ys = []

            botnet_count = 0
            bg_count = 0
            skipped = 0

            with open(window) as f:
                try:
                    reader = csv.DictReader(f)
                    for flow in reader:
                        try:
                            if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                                skipped += 1
                                continue

                            flow_id, x = parse_argus_flow(flow, self.features_list)
                        except Exception, e:
                            skipped += 1
                            continue

                        if int(window.split(".")[0]) < infection_start:
                            bg_count += 1
                            flow_ids.append(flow_id)
                            xs.append(x)
                            ys.append(0)
                        else:
                            if flow_id in self.botnet_flows:
                                botnet_count += 1
                                xs.append(x)
                                ys.append(1)  # ys.append([1])
                                flow_ids.append(flow_id)
                            elif flow_id in self.normal_flows or flow_id in self.background_flows:
                                bg_count += 1
                                xs.append(x)
                                ys.append(0)
                                flow_ids.append(flow_id)
                            else:
                                skipped += 1
                                continue

                    xs = np.nan_to_num(xs)
                    ys = np.array(ys)

                    # scaled_xs = StandardScaler().fit_transform(xs)

                    predicted = clf.predict(xs)
                    acc = accuracy_score(predicted, ys)

                    TP = 0
                    TN = 0
                    FP = 0
                    FN = 0

                    for y_actual, y_pred in zip(ys, predicted):
                        if y_actual == 0 and y_pred == 0:
                            TN += 1
                        elif y_actual == 0 and y_pred == 1:
                            FP += 1
                        elif y_actual == 1 and y_pred == 0:
                            FN += 1
                        elif y_actual == 1 and y_pred == 1:
                            TP += 1

                    print("window: " + str(window.split(".")[0] + "\tflows: " + str(bg_count + botnet_count) +
                          "\tbotnet: " + str(botnet_count) + "\tbg: " + str(bg_count) + "\t\tacc: " + str(acc)) +
                          ("\t\ttrue positive: " + str(TP)) + "\t\tfalse positive: " + str(FP) + "\t\tfalse negative: " +
                          str(FN) + "\t\ttrue negative: " + str(TN))
                except Exception, e:
                    print("error on window: " + window)


# -----------------------------------------------------------------------------------------------------
# OLD CODE:

'''
Parse an Argus flow for the ISCX dataset, which doesn't come with labelled files but has a list of
infected IPs (in constants)
Currently not much use
'''
def parse_iscx(filename, features_list=ARGUS_FIELDS):
    background_count = 0
    botnet_count = 0
    skipped = 0

    flows = []
    xs = []
    ys = []

    with open(filename) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            if (flow['Dir'] == '') or (int(flow['SrcPkts']) + int(flow['DstPkts']) == 0):
                skipped += 1
                continue

            try:
                flow_id, x = parse_argus_flow(flow, features_list)
                #flow_id = (flow_id[0], flow_id[4])
            except Exception, e:
                skipped += 1
                continue

            flows.append(flow_id)
            xs.append(x)

            if (flow['Dir'].strip in {'<->', '<?>'}) and (flow['SrcAddr'] in constants.ISCX_INFECTED_HOSTS or flow['DstAddr'] in constants.ISCX_INFECTED_HOSTS):
                ys.append(1)
                botnet_count += 1
            elif (flow['Dir'].strip() == '->') and flow['SrcAddr'] in constants.ISCX_INFECTED_HOSTS:
                ys.append(1)
                botnet_count += 1
            elif (flow['Dir'].strip() == '<-') and flow['DstAddr'] in constants.ISCX_INFECTED_HOSTS:
                ys.append(1)
                botnet_count += 1
            else:
                ys.append(0)
                background_count += 1

    print("Parsed total " + str(botnet_count + background_count) + " flows, skipped " + str(skipped))
    print("Total " + str(botnet_count) + " botnet, " + str(background_count) + " non-botnet")

    xs = np.nan_to_num(xs)
    return (flows, np.array(xs), np.array(ys))


# TRANALYZER PARSER BELOW (no longer used since we have shifted to Argus)

# These fields are used as flow ID
FLOW_IDENTIFIERS = {2:"flowInd", 3:"flowStat", 11:"srcIP4", 12:"srcIPCC", 13:"srcPort", 14:"dstIP4",
                    15:"dstIPCC", 16:"dstPort", 17:"l4Proto"}

# These fields are discarded during parsing (usually because non-numeric or other issues)
IGNORE_PARAMS = {4:"unixTimeFirst", 5:"unixTimeLast", 8:"numHdrs", 9:"hdrDesc", 10:"ethVlanID",
                 31:"Ps_Iat_Cnt_PsCnt_IatCnt"}

HEADERS_FILE = "../etc/tranalyzer_headers.txt"

DELIMITER = "\t"
INTER_DELIMITER = "_"
COMMENT = "#"

# Initially a netflow line is parsed as a dictionary of all the fields.
# Given a flow ID, which is an array of values of the fields listed in FLOW_IDENTIFIERS in the order
# they were parsed. For Tranalyzer netflow files, the source IPv4 is the third element and the destination
# IPv4 is the 6th element.
# Returns source IP and destination IP as a tuple for a given flow ID.
def get_t_src_dst(flow_id):
    return (flow_id[2], flow_id[5])

# Returns a list of the fields generated by Argus, given a provided header file.
def headers():
    headers = []
    with open(HEADERS_FILE, "r") as fin:
        for line in fin:
            if line[0] == COMMENT:
                continue

            columns = line.split(DELIMITER)
            if int(columns[0]) not in FLOW_IDENTIFIERS and int(columns[0]) not in IGNORE_PARAMS:
                headers.append(columns[2])
    return headers

# Parses a single line of the netflow file. For Argus, these files will be tab-delimited
# Returns the flow ID as an array and the feature vector as an array of numbers, in a tuple
# i.e. (flow_ID, feature_vector)
def parse_flow(line):
    flow_id = []
    features = []
    _features = line.split(DELIMITER)

    for i, feature in enumerate(_features, 1):
        if i in IGNORE_PARAMS:
            # if feature should be skipped over, skip over
            continue
        elif i == 1: # flow dir
            # second field is direction, which is output as A or B. we turn this into a numeric value
            # i.e. 1 if A and 2 if B
            features.append(1 if feature == "A" else 2)
        elif i in FLOW_IDENTIFIERS:
            # if feature is part of flow ID, append it to flow ID
            flow_id.append(feature)
        else:
            # otherwise, if feature is a regular feature, append it to the vector and treat as a numeric
            # value
            features.append(float(feature))

    # return the flow ID and the feature vector of that line
    return (tuple(flow_id), features)

# Parses a single line of a Tranalyzer netflow file (not used anymore since we have shifted from
# Tranalyzer to Argus)
def tparse_single(filename):
    flow_ids = []
    xs = []

    with open(filename, "r") as fin:
        header = fin.readline()
        for line in fin:
            try:
                flow_id, features = parse_flow(line)
                flow_ids.append(flow_id)
                xs.append(features)
            except Exception, e:
                continue

    return (flow_ids, np.array(xs))


def tparse_combined(all_file, botnet_file, limit_normal=True, limit_val=9):
    xs = []
    ys = []
    flows = []
    botnet_flows = set()

    with open(botnet_file, "r") as fin:
        header = fin.readline()
        for line in fin:
            try:
                flow_id, features = parse_flow(line)
                botnet_flows.add(flow_id)
                flows.append(flow_id)
                xs.append(features)
                ys.append([1])
            except Exception, e:
                continue

    print("Total botnet flows: " + str(len(xs)))

    limit = limit_val*len(xs)
    with open(all_file, "r") as fin:
        header = fin.readline()
        for line in fin:
            if limit_normal == True and limit == 0:
                break
            try:
                flow_id, features = parse_flow(line)
                if flow_id not in botnet_flows:
                    flows.append(flow_id)
                    xs.append(features)
                    ys.append([0])
                    limit -= 1
            except Exception, e:
                continue

    print("Total flows: " + str(len(xs)))

    all_data = list(zip(flows, xs, ys))
    random.shuffle(all_data)
    flows, xs, ys = zip(*all_data)

    return (flows, np.array(xs), np.array(ys))

# Improved tranalyzer parser

TRANALYZER_FLOW_ID = ['srcIP4', 'srcPort', 'dstIP4', 'dstPort', 'hdrDesc']
TRANALYZER_FIELDS = ['dir', 'duration', 'numPktsSnt', 'numPktsRcvd', 'numBytesSnt', 'numBytesRcvd',
                     'minPktSz', 'maxPktSz', 'avePktSize', 'stdPktSize', 'pktps', 'bytps', 'pktAsm', 'bytAsm',
                     'tCnt', 'MinPl', 'MaxPl', 'MeanPl', 'LowQuartilePl', 'MedianPl', 'UppQuartilePl', 'IqdPl',
                     'ModePl', 'RangePl', 'StdPl', 'RobStdPl', 'SkewPl', 'ExcPl', 'MinIat', 'MaxIat', 'MeanIat',
                     'LowQuartileIat', 'MedianIat', 'UppQuartileIat', 'IqdIat', 'ModeIat', 'RangeIat', 'StdIat',
                     'RobStdIat', 'SkewIat', 'ExcIat']

def get_tranalyzer_flow_id(flow):
    proto = flow['hdrDesc'].split(':')[-1]
    return (flow["srcIP4"], flow["srcPort"], flow["dstIP4"], flow["dstPort"], proto)

def parse_tranalyzer_flow(flow):
    features = []
    for field in TRANALYZER_FIELDS:
        if field == 'dir':
            features.append(1 if flow['dir'] == 'A' else 2)
        else:
            features.append(float(flow[field]))
    return (get_tranalyzer_flow_id(flow), features)

def parse_tranalyzer(labels, filename):
    botnet_flows = set()
    bg_flows = set()

    print("Opening labelled Argus binetflow...")
    total = 0
    with open(labels) as f:
        reader = csv.DictReader(f)
        for flow in reader:
            total += 1
            flow_id = get_argus_flow_id(flow)

            if 'From-Botnet' in flow['Label']:
                botnet_flows.add(flow_id)
            else:
                bg_flows.add(flow_id)

    print("Parsed total " + str(total) + " flows")
    print("Total " + str(len(botnet_flows)) + " unique From-Botnet flow IDs")
    print("Total " + str(len(bg_flows)) + " unique Background and Normal flow IDs")

    print("Opening Tranalyzer2 flow record...")
    total = 0
    botnet_count = 0
    skipped = 0
    flow_ids = []
    xs = []
    ys = []
    with open(filename) as f:
        header = f.readline().strip("%").strip().split(DELIMITER)
        #print(header)
        for line in f:
            flow = dict(zip(header, line.strip().split(DELIMITER)))
        #reader = csv.DictReader(f, delimiter='\t', quoting=csv.QUOTE_NONE)
        #for flow in reader:
            total += 1

            try:
                flow_id, x = parse_tranalyzer_flow(flow)
                # print(flow_id, x)
            except Exception, e:
                skipped += 1
                continue

            if flow_id in botnet_flows:
                xs.append(x)
                ys.append([1])
                flow_ids.append(flow_id)
                botnet_count += 1
            elif flow_id in bg_flows:
                xs.append(x)
                ys.append([0])
                flow_ids.append(flow_id)
            else:
                skipped += 1
                continue

    print("Parsed total " + str(total) + " flows, skipped " + str(skipped))
    print("Final dataset contains " + str(len(xs)) + " flows, " + str(botnet_count) + " botnet flows")

    xs = np.array(xs)
    xs = np.nan_to_num(xs)
    ys = np.array(ys)

    # p = np.random.permutation(len(xs))
    all_data = list(zip(flow_ids, xs, ys))
    random.shuffle(all_data)
    flow_ids, xs, ys = zip(*all_data)

    return (flow_ids, xs, ys)